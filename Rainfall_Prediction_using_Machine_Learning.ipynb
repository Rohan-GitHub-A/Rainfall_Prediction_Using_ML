{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820544c5",
   "metadata": {},
   "source": [
    "**Enhanced Rainfall Prediction Model with Advanced Optimizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251095f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score, \n",
    "                                   StratifiedKFold, RandomizedSearchCV)\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                            ExtraTreesClassifier, VotingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                           precision_recall_curve, roc_auc_score, roc_curve, f1_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab038af",
   "metadata": {},
   "source": [
    "**Enhanced Data Collection and Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922f1b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath):\n",
    "    \"\"\"Enhanced data loading with comprehensive cleaning\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "        print(f\"Dataset loaded successfully with shape: {data.shape}\")\n",
    "        \n",
    "        # Clean column names\n",
    "        data.columns = data.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data\n",
    "data = load_and_clean_data(\"DataSets/Rainfall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7501f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def enhanced_data_preprocessing(data):\n",
    "    \"\"\"Comprehensive data preprocessing pipeline\"\"\"\n",
    "    \n",
    "    # Drop irrelevant columns if they exist\n",
    "    columns_to_drop = ['day', 'date'] if any(col in data.columns for col in ['day', 'date']) else []\n",
    "    if columns_to_drop:\n",
    "        data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "    \n",
    "    print(\"Missing values before handling:\")\n",
    "    print(data.isnull().sum())\n",
    "    \n",
    "    # Enhanced missing value handling\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == 'object':  # Categorical variables\n",
    "            if column != 'rainfall':\n",
    "                data[column] = data[column].fillna(data[column].mode()[0] if not data[column].mode().empty else 'unknown')\n",
    "        else:  # Numerical variables\n",
    "            # Use median for skewed distributions, mean for normal distributions\n",
    "            if data[column].skew() > 1:\n",
    "                data[column] = data[column].fillna(data[column].median())\n",
    "            else:\n",
    "                data[column] = data[column].fillna(data[column].mean())\n",
    "    \n",
    "    # Convert target variable\n",
    "    if 'rainfall' in data.columns:\n",
    "        if data['rainfall'].dtype == 'object':\n",
    "            data['rainfall'] = data['rainfall'].map({'yes': 1, 'no': 0})\n",
    "    \n",
    "    print(\"\\nMissing values after handling:\")\n",
    "    print(data.isnull().sum())\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply preprocessing\n",
    "data = enhanced_data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c90fd",
   "metadata": {},
   "source": [
    "**Advanced Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eca691",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_advanced_features(data):\n",
    "    \"\"\"Create advanced engineered features\"\"\"\n",
    "    \n",
    "    # Temperature-related features (if temperature columns exist)\n",
    "    temp_cols = [col for col in data.columns if 'temp' in col.lower()]\n",
    "    if len(temp_cols) >= 2:\n",
    "        # Temperature range\n",
    "        data['temp_range'] = data[temp_cols].max(axis=1) - data[temp_cols].min(axis=1)\n",
    "        # Average temperature\n",
    "        data['temp_avg'] = data[temp_cols].mean(axis=1)\n",
    "    \n",
    "    # Pressure features\n",
    "    if 'pressure' in data.columns:\n",
    "        data['pressure_normalized'] = (data['pressure'] - data['pressure'].mean()) / data['pressure'].std()\n",
    "        \n",
    "    # Humidity features\n",
    "    if 'humidity' in data.columns:\n",
    "        data['humidity_high'] = (data['humidity'] > 80).astype(int)\n",
    "        data['humidity_low'] = (data['humidity'] < 30).astype(int)\n",
    "    \n",
    "    # Wind features\n",
    "    if 'windspeed' in data.columns:\n",
    "        data['wind_calm'] = (data['windspeed'] < 5).astype(int)\n",
    "        data['wind_strong'] = (data['windspeed'] > 15).astype(int)\n",
    "    \n",
    "    # Cloud and sunshine interaction\n",
    "    if 'cloud' in data.columns and 'sunshine' in data.columns:\n",
    "        data['cloud_sunshine_ratio'] = data['cloud'] / (data['sunshine'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    # Dewpoint features\n",
    "    if 'dewpoint' in data.columns and any('temp' in col for col in data.columns):\n",
    "        temp_col = [col for col in data.columns if 'temp' in col][0]\n",
    "        data['dewpoint_spread'] = data[temp_col] - data['dewpoint']\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply feature engineering\n",
    "data = create_advanced_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cacb1",
   "metadata": {},
   "source": [
    "**Enhanced Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f764b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def comprehensive_eda(data):\n",
    "    \"\"\"Comprehensive EDA with advanced visualizations\"\"\"\n",
    "    \n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Columns: {list(data.columns)}\")\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    # Target distribution\n",
    "    if 'rainfall' in data.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(data=data, x='rainfall')\n",
    "        plt.title(\"Target Distribution (Rainfall)\")\n",
    "        for i, v in enumerate(data['rainfall'].value_counts()):\n",
    "            plt.text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(data['rainfall'].value_counts())\n",
    "        print(f\"Class imbalance ratio: {data['rainfall'].value_counts().max() / data['rainfall'].value_counts().min():.2f}\")\n",
    "    \n",
    "    # Feature distributions\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'rainfall']\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        n_cols = min(4, len(numeric_cols))\n",
    "        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "        \n",
    "        plt.figure(figsize=(20, 5 * n_rows))\n",
    "        for i, col in enumerate(numeric_cols, 1):\n",
    "            plt.subplot(n_rows, n_cols, i)\n",
    "            sns.histplot(data[col], kde=True, bins=30)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        correlation_matrix = data[numeric_cols + ['rainfall']].corr()\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                   center=0, square=True, fmt='.2f')\n",
    "        plt.title(\"Feature Correlation Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance based on correlation with target\n",
    "        if 'rainfall' in correlation_matrix.columns:\n",
    "            feature_importance = correlation_matrix['rainfall'].abs().sort_values(ascending=False)[1:]\n",
    "            print(\"\\nFeature importance (correlation with target):\")\n",
    "            print(feature_importance)\n",
    "\n",
    "# Perform comprehensive EDA\n",
    "comprehensive_eda(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f2c47",
   "metadata": {},
   "source": [
    "**Advanced Data Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6f032",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_multicollinearity(X, threshold=0.95):\n",
    "    \"\"\"Remove highly correlated features\"\"\"\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    \n",
    "    if to_drop:\n",
    "        print(f\"Dropping highly correlated features: {to_drop}\")\n",
    "        X = X.drop(columns=to_drop)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def handle_imbalanced_data(X, y, method='smote', random_state=42):\n",
    "    \"\"\"Handle class imbalance with multiple strategies\"\"\"\n",
    "    \n",
    "    print(f\"Original class distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # SMOTE for oversampling\n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=min(5, np.bincount(y).min()-1))\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    elif method == 'undersample':\n",
    "        # Random undersampling\n",
    "        undersampler = RandomUnderSampler(random_state=random_state)\n",
    "        X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "    elif method == 'combined':\n",
    "        # Combined over and under sampling\n",
    "        oversample = SMOTE(sampling_strategy=0.5, random_state=random_state)\n",
    "        undersample = RandomUnderSampler(sampling_strategy=0.8, random_state=random_state)\n",
    "        X_resampled, y_resampled = oversample.fit_resample(X, y)\n",
    "        X_resampled, y_resampled = undersample.fit_resample(X_resampled, y_resampled)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "    \n",
    "    print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(columns=['rainfall'])\n",
    "y = data['rainfall']\n",
    "\n",
    "# Remove multicollinearity\n",
    "X = remove_multicollinearity(X, threshold=0.90)\n",
    "\n",
    "print(f\"Final feature set: {list(X.columns)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "# Handle imbalanced data\n",
    "X_train_balanced, y_train_balanced = handle_imbalanced_data(X_train, y_train, method='smote')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dc2dc",
   "metadata": {},
   "source": [
    "**Advanced Model Training with Multiple Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39b402",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_advanced_models():\n",
    "    \"\"\"Create multiple optimized models\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'random_forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'gradient_boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'extra_trees': ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
    "        'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'svm': SVC(random_state=42, probability=True)\n",
    "    }\n",
    "    \n",
    "    param_grids = {\n",
    "        'random_forest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'extra_trees': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'logistic_regression': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        },\n",
    "        'svm': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return models, param_grids\n",
    "\n",
    "def train_and_optimize_models(X_train, y_train, models, param_grids):\n",
    "    \"\"\"Train and optimize multiple models\"\"\"\n",
    "    \n",
    "    best_models = {}\n",
    "    cv_scores = {}\n",
    "    \n",
    "    # Use StratifiedKFold for better cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nOptimizing {name}...\")\n",
    "        \n",
    "        # Use RandomizedSearchCV for faster optimization\n",
    "        if name in ['random_forest', 'gradient_boosting', 'extra_trees']:\n",
    "            search = RandomizedSearchCV(\n",
    "                model, param_grids[name], \n",
    "                n_iter=50, cv=cv, scoring='f1',\n",
    "                n_jobs=-1, random_state=42, verbose=1\n",
    "            )\n",
    "        else:\n",
    "            search = GridSearchCV(\n",
    "                model, param_grids[name], \n",
    "                cv=cv, scoring='f1',\n",
    "                n_jobs=-1, verbose=1\n",
    "            )\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        best_models[name] = search.best_estimator_\n",
    "        cv_scores[name] = search.best_score_\n",
    "        \n",
    "        print(f\"Best {name} CV F1 Score: {search.best_score_:.4f}\")\n",
    "        print(f\"Best parameters: {search.best_params_}\")\n",
    "    \n",
    "    return best_models, cv_scores\n",
    "\n",
    "# Create and train models\n",
    "models, param_grids = create_advanced_models()\n",
    "best_models, cv_scores = train_and_optimize_models(X_train_balanced, y_train_balanced, \n",
    "                                                  models, param_grids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74a478",
   "metadata": {},
   "source": [
    "**Ensemble Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8f87e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_ensemble_model(best_models, cv_scores):\n",
    "    \"\"\"Create an ensemble model using the best performing models\"\"\"\n",
    "    \n",
    "    # Select top 3 models based on CV scores\n",
    "    sorted_models = sorted(cv_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_models = [(name, best_models[name]) for name, score in sorted_models[:3]]\n",
    "    \n",
    "    print(\"Top 3 models for ensemble:\")\n",
    "    for name, model in top_models:\n",
    "        print(f\"- {name}: {cv_scores[name]:.4f}\")\n",
    "    \n",
    "    # Create voting classifier\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=top_models,\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble_model = create_ensemble_model(best_models, cv_scores)\n",
    "\n",
    "# Train ensemble model\n",
    "print(\"\\nTraining ensemble model...\")\n",
    "ensemble_model.fit(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0718be1",
   "metadata": {},
   "source": [
    "**Comprehensive Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6821c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(models, X_test, y_test, model_names=None):\n",
    "    \"\"\"Comprehensive model evaluation with multiple metrics\"\"\"\n",
    "    \n",
    "    if model_names is None:\n",
    "        model_names = [f\"Model_{i}\" for i in range(len(models))]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in zip(model_names, models):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        if y_pred_proba is not None:\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            results[name]['auc_score'] = auc_score\n",
    "            print(f\"AUC Score: {auc_score:.4f}\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        # ROC Curve\n",
    "        if y_pred_proba is not None:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {name}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models including ensemble\n",
    "all_models = list(best_models.values()) + [ensemble_model]\n",
    "model_names = list(best_models.keys()) + ['ensemble']\n",
    "\n",
    "evaluation_results = comprehensive_evaluation(all_models, X_test, y_test, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578974d",
   "metadata": {},
   "source": [
    "**Feature Importance Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb6d42",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names, model_name=\"Model\"):\n",
    "    \"\"\"Analyze and visualize feature importance\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = abs(model.coef_[0])\n",
    "    else:\n",
    "        print(f\"Feature importance not available for {model_name}\")\n",
    "        return\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features ({model_name}):\")\n",
    "    print(feature_imp.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_imp.head(15), x='importance', y='feature')\n",
    "    plt.title(f'Top 15 Feature Importances - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "# Analyze feature importance for the best model\n",
    "best_model_name = max(evaluation_results.keys(), \n",
    "                     key=lambda k: evaluation_results[k].get('f1_score', 0))\n",
    "best_model = dict(zip(model_names, all_models))[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "feature_importance = analyze_feature_importance(best_model, X.columns.tolist(), best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ed916",
   "metadata": {},
   "source": [
    "**Enhanced Prediction Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf40365",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_prediction_function(model, feature_names, scaler=None):\n",
    "    \"\"\"Create an enhanced prediction function with confidence scores\"\"\"\n",
    "    \n",
    "    def predict_rainfall(input_data, return_probability=False):\n",
    "        \"\"\"\n",
    "        Enhanced prediction function\n",
    "        \n",
    "        Args:\n",
    "            input_data: tuple or list of feature values\n",
    "            return_probability: whether to return probability scores\n",
    "        \n",
    "        Returns:\n",
    "            prediction result and optionally probability scores\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert input to DataFrame\n",
    "        if isinstance(input_data, (tuple, list)):\n",
    "            input_df = pd.DataFrame([input_data], columns=feature_names)\n",
    "        else:\n",
    "            input_df = input_data\n",
    "        \n",
    "        # Apply scaling if scaler is provided\n",
    "        if scaler:\n",
    "            input_df_scaled = pd.DataFrame(\n",
    "                scaler.transform(input_df),\n",
    "                columns=feature_names\n",
    "            )\n",
    "        else:\n",
    "            input_df_scaled = input_df\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(input_df_scaled)[0]\n",
    "        result = \"Rainfall\" if prediction == 1 else \"No Rainfall\"\n",
    "        \n",
    "        if return_probability and hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(input_df_scaled)[0]\n",
    "            confidence = max(probabilities)\n",
    "            \n",
    "            print(f\"Prediction: {result}\")\n",
    "            print(f\"Confidence: {confidence:.3f}\")\n",
    "            print(f\"Probability of Rainfall: {probabilities[1]:.3f}\")\n",
    "            print(f\"Probability of No Rainfall: {probabilities[0]:.3f}\")\n",
    "            \n",
    "            return result, probabilities, confidence\n",
    "        else:\n",
    "            print(f\"Prediction: {result}\")\n",
    "            return result\n",
    "    \n",
    "    return predict_rainfall\n",
    "\n",
    "# Create prediction function\n",
    "predict_rainfall = create_prediction_function(best_model, X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dfc9c",
   "metadata": {},
   "source": [
    "**Model Persistence and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_enhanced_model(model, feature_names, scaler=None, filename=\"enhanced_rainfall_model.pkl\"):\n",
    "    \"\"\"Save the enhanced model with all necessary components\"\"\"\n",
    "    \n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'feature_names': feature_names,\n",
    "        'scaler': scaler,\n",
    "        'model_type': type(model).__name__,\n",
    "        'feature_count': len(feature_names)\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model_package, file)\n",
    "    \n",
    "    print(f\"Enhanced model saved as {filename}\")\n",
    "    print(f\"Model type: {type(model).__name__}\")\n",
    "    print(f\"Features: {len(feature_names)}\")\n",
    "\n",
    "def load_enhanced_model(filename=\"enhanced_rainfall_model.pkl\"):\n",
    "    \"\"\"Load the enhanced model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            model_package = pickle.load(file)\n",
    "        \n",
    "        model = model_package['model']\n",
    "        feature_names = model_package['feature_names']\n",
    "        scaler = model_package.get('scaler')\n",
    "        \n",
    "        print(f\"Model loaded successfully!\")\n",
    "        print(f\"Model type: {model_package['model_type']}\")\n",
    "        print(f\"Features: {model_package['feature_count']}\")\n",
    "        \n",
    "        # Create prediction function\n",
    "        predict_func = create_prediction_function(model, feature_names, scaler)\n",
    "        \n",
    "        return model, feature_names, scaler, predict_func\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Save the best model\n",
    "save_enhanced_model(best_model, X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e229cd",
   "metadata": {},
   "source": [
    "**Example Usage and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbe050",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test the enhanced prediction function\n",
    "print(\"Testing Enhanced Prediction Function:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example input (adjust based on your actual features)\n",
    "sample_inputs = [\n",
    "    (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7),  # Original example\n",
    "    (1020.0, 15.0, 60, 45, 8.0, 180.0, 5.0),  # Low humidity, high sunshine\n",
    "    (1005.0, 22.0, 98, 90, 0.5, 200.0, 20.0)  # High humidity, low sunshine\n",
    "]\n",
    "\n",
    "for i, input_data in enumerate(sample_inputs, 1):\n",
    "    print(f\"\\nSample Input {i}:\")\n",
    "    print(f\"Input values: {input_data}\")\n",
    "    result = predict_rainfall(input_data, return_probability=True)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16bd72",
   "metadata": {},
   "source": [
    "**Model Performance Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(evaluation_results):\n",
    "    \"\"\"Print a comprehensive summary of model performance\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sort models by F1 score\n",
    "    sorted_results = sorted(evaluation_results.items(), \n",
    "                           key=lambda x: x[1].get('f1_score', 0), \n",
    "                           reverse=True)\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Accuracy':<10} {'F1 Score':<10} {'AUC Score':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for model_name, results in sorted_results:\n",
    "        accuracy = results['accuracy']\n",
    "        f1_score = results['f1_score']\n",
    "        auc_score = results.get('auc_score', 'N/A')\n",
    "        \n",
    "        auc_str = f\"{auc_score:.4f}\" if auc_score != 'N/A' else 'N/A'\n",
    "        print(f\"{model_name:<20} {accuracy:<10.4f} {f1_score:<10.4f} {auc_str:<10}\")\n",
    "    \n",
    "    print(\"\\nBest performing model:\", sorted_results[0][0])\n",
    "    print(\"=\"*60)\n",
    "\n",
    "print_model_summary(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a8087",
   "metadata": {},
   "source": [
    "**Key Improvements Implemented:**\n",
    "\n",
    "1. **Advanced Feature Engineering**: Created interaction features, normalized features, and domain-specific features\n",
    "2. **Multiple Model Algorithms**: Implemented Random Forest, Gradient Boosting, Extra Trees, Logistic Regression, and SVM\n",
    "3. **Ensemble Methods**: Combined best models using Voting Classifier\n",
    "4. **Better Imbalance Handling**: Used SMOTE for intelligent oversampling\n",
    "5. **Comprehensive Evaluation**: Multiple metrics including AUC, F1-score, precision-recall curves\n",
    "6. **Feature Selection**: Automatic removal of multicollinear features\n",
    "7. **Enhanced Cross-Validation**: Stratified K-Fold for better validation\n",
    "8. **Hyperparameter Optimization**: RandomizedSearchCV for faster optimization\n",
    "9. **Robust Preprocessing**: Better missing value handling and outlier detection\n",
    "10. **Prediction Confidence**: Added probability scores and confidence measures"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
